import os
import json
import time
from datetime import datetime, timedelta
import re # æ­£è¦è¡¨ç¾ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

# --- Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ (Selenium) é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

# --- Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ (Requests + BeautifulSoup) é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
import requests
from bs4 import BeautifulSoup
# from urllib.parse import quote # URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ç”¨ (requestsãŒè‡ªå‹•ã§å‡¦ç†ã™ã‚‹å ´åˆãŒå¤šã„ãŒã€å¿µã®ãŸã‚)

# gspreadé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import gspread

# å…±é€šè¨­å®š
KEYWORD = "æ—¥ç”£"
SPREADSHEET_ID = "1RglATeTbLU1SqlfXnNToJqhXLdNoHCdePldioKDQgU8" # ã”è‡ªèº«ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆID


# --- Googleãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–¢æ•° (æ—¢å­˜) ---
def get_google_news_with_selenium(keyword: str) -> list[dict]:
    """
    Seleniumã‚’ä½¿ç”¨ã—ã¦Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—ã—ã¾ã™ã€‚
    """
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        print("ChromeDriverã‚’èµ·å‹•ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: ChromeDriverã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸã€‚{e}")
        return []

    url = f"https://news.google.com/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja"
    print(f"ã‚¢ã‚¯ã‚»ã‚¹URL (Google): {url}")
    
    articles_data = []
    try:
        driver.get(url)
        time.sleep(5)

        for _ in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

        articles = driver.find_elements(By.TAG_NAME, "article")
        print(f"æ¤œå‡ºã•ã‚ŒãŸè¨˜äº‹è¦ç´ ã®æ•° (Google): {len(articles)}")

        for i, article_elem in enumerate(articles):
            try:
                title_tag = article_elem.find_element(By.CSS_SELECTOR, "a.JtKRv")
                date_tag = article_elem.find_element(By.CSS_SELECTOR, "time.hvbAAd")
                source_tag = article_elem.find_element(By.CSS_SELECTOR, "div.vr1PYe")

                title = title_tag.text.strip()
                date_utc_str = date_tag.get_attribute("datetime")
                url = title_tag.get_attribute("href")
                source = source_tag.text.strip() if source_tag else "N/A"

                if title and date_utc_str and url:
                    utc_dt = datetime.strptime(date_utc_str, "%Y-%m-%dT%H:%M:%SZ")
                    jst_dt = utc_dt + timedelta(hours=9)
                    formatted_date = jst_dt.strftime("%Y/%#m/%#d %H:%M") # Windowsç”¨
                    # ãã®ä»–ã®OSã®å ´åˆ: formatted_date = jst_dt.strftime("%Y/%-m/%-d %H:%M")

                    full_url = "https://news.google.com" + url[1:] if url.startswith("./articles/") else url

                    articles_data.append({
                        'ã‚¿ã‚¤ãƒˆãƒ«': title,
                        'URL': full_url,
                        'æŠ•ç¨¿æ—¥': formatted_date,
                        'å¼•ç”¨å…ƒ': source
                    })
            except Exception as e:
                # print(f"Googleè¨˜äº‹è¦ç´  {i} ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                continue
        return articles_data
    except Exception as e:
        print(f"Googleãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []
    finally:
        if driver:
            driver.quit()


# --- Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–¢æ•° (æ–°è¦è¿½åŠ ) ---
def get_yahoo_news_with_requests(keyword: str) -> list[dict]:
    """
    Requestsã¨BeautifulSoupã‚’ä½¿ç”¨ã—ã¦Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—ã—ã¾ã™ã€‚
    """
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36"} # æœ€æ–°ã®User-Agentã‚’æ¨å¥¨
    
    url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    print(f"ã‚¢ã‚¯ã‚»ã‚¹URL (Yahoo!): {url}")
    
    articles_data = []
    try:
        res = requests.get(url, headers=headers, timeout=10) # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã‚’è¿½åŠ 
        res.raise_for_status() # HTTPã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã«ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹
        soup = BeautifulSoup(res.text, "html.parser")

        # Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚»ãƒ¬ã‚¯ã‚¿ã¯é »ç¹ã«å¤‰ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€æ³¨æ„æ·±ãç¢ºèª
        # ä¸»ã« li[data-tn-screen="results-item"] ã‚’ä½¿ç”¨
        article_blocks = soup.find_all("li", attrs={"data-tn-screen": "results-item"})

        if not article_blocks:
            # ã‚‚ã—ä¸Šè¨˜ã‚»ãƒ¬ã‚¯ã‚¿ã§å–å¾—ã§ããªã‘ã‚Œã°ã€å‚è€ƒãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚‚è©¦ã™
            article_blocks = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
            if not article_blocks:
                print("âš ï¸ Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹: è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚»ãƒ¬ã‚¯ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                return []

        print(f"æ¤œå‡ºã•ã‚ŒãŸè¨˜äº‹è¦ç´ ã®æ•° (Yahoo!): {len(article_blocks)}")

        for i, article in enumerate(article_blocks):
            try:
                # ã‚¿ã‚¤ãƒˆãƒ«: aã‚¿ã‚°ã§ class="sc-3ls169-0..."
                title_tag = article.find("a", class_=re.compile("sc-3ls169-0.+"))
                title = title_tag.text.strip() if title_tag else ""

                link_tag = article.find("a", href=True)
                link = link_tag["href"] if link_tag else ""

                # æŠ•ç¨¿æ—¥: timeã‚¿ã‚°
                time_tag = article.find("time")
                date_str = time_tag.text.strip() if time_tag else ""

                # æŠ•ç¨¿æ—¥æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›´
                formatted_date = ""
                if date_str:
                    date_str = re.sub(r'\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)', '', date_str).strip() # æ‹¬å¼§å†…ã®æ›œæ—¥ã‚’å‰Šé™¤
                    try:
                        # 'YYYY/M/D H:MM' å½¢å¼ã«å¤‰æ› (%m, %dã¯ã‚¼ãƒ­åŸ‹ã‚ãªã—ã«ã‚‚å¯¾å¿œ)
                        dt_obj = datetime.strptime(date_str, "%Y/%m/%d %H:%M")
                        formatted_date = dt_obj.strftime("%Y/%#m/%#d %H:%M") # Windowsã®å ´åˆ
                        # ãã®ä»–ã®OSã®å ´åˆ: formatted_date = dt_obj.strftime("%Y/%-m/%-d %H:%M")
                    except ValueError:
                        # å¤‰æ›ã§ããªã„å ´åˆã¯å…ƒã®æ–‡å­—åˆ—ã‚’ãã®ã¾ã¾ä½¿ç”¨
                        formatted_date = date_str

                # ã‚½ãƒ¼ã‚¹ã®æŠ½å‡º
                # data-by-textå±æ€§ã‚’æŒã¤divã€ã¾ãŸã¯å‚è€ƒãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ã‚»ãƒ¬ã‚¯ã‚¿
                source_tag = article.find("div", attrs={"data-by-text": True})
                source_text = source_tag["data-by-text"].strip() if source_tag else "N/A"
                
                # ã‚‚ã—ä¸Šè¨˜ã§å–ã‚Œãªã‘ã‚Œã°ã€å‚è€ƒãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚‚è©¦ã™
                if source_text == "N/A" or not source_text:
                    source_tag_alt = article.find("div", class_="sc-n3vj8g-0") # yoLqH ã¯å‹•çš„ã‚¯ãƒ©ã‚¹åãªã®ã§å‰Šé™¤
                    if source_tag_alt:
                        inner_source_tag_alt = source_tag_alt.find("span", class_=re.compile("sc-110wjhy-8.+")) # bsEjY ã‚‚å‹•çš„ã‚¯ãƒ©ã‚¹åãªã®ã§å‰Šé™¤
                        if inner_source_tag_alt:
                            source_text = inner_source_tag_alt.text.strip() if inner_source_tag_alt else "N/A"


                if not title or not link:
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‹URLãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue

                articles_data.append({
                    'ã‚¿ã‚¤ãƒˆãƒ«': title,
                    'URL': link,
                    'æŠ•ç¨¿æ—¥': formatted_date,
                    'å¼•ç”¨å…ƒ': source_text
                })
                time.sleep(0.3) # é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹ã‚’é¿ã‘ã‚‹ãŸã‚ã®é…å»¶
            except Exception as e:
                # print(f"Yahooè¨˜äº‹è¦ç´  {i} ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                continue
        return articles_data
    except requests.exceptions.RequestException as e:
        print(f"Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []
    except Exception as e:
        print(f"Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []


# --- ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿é–¢æ•° (æ—¢å­˜ + ã‚½ãƒ¼ãƒˆãƒ­ã‚¸ãƒƒã‚¯å¼·åŒ–) ---
def write_to_spreadsheet(articles: list[dict], spreadsheet_id: str, worksheet_name: str):
    """
    å–å¾—ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿ã¾ã™ã€‚
    æ—¢å­˜ã®URLã‚’ãƒã‚§ãƒƒã‚¯ã—ã€æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’è¿½è¨˜ã—ã¾ã™ã€‚
    è¨˜äº‹ã¯æŠ•ç¨¿æ—¥ã®æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆã•ã‚Œã¦ã‹ã‚‰è¿½è¨˜ã•ã‚Œã¾ã™ã€‚

    Args:
        articles (list[dict]): æ›¸ãè¾¼ã‚€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ãƒªã‚¹ãƒˆã€‚
        spreadsheet_id (str): æ›¸ãè¾¼ã¿å…ˆã®Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã€‚
        worksheet_name (str): æ›¸ãè¾¼ã¿å…ˆã®ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆåã€‚
    """
    credentials_json_str = os.environ.get('GCP_SERVICE_ACCOUNT_KEY')
    credentials = None

    if not credentials_json_str:
        print("GCP_SERVICE_ACCOUNT_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒãƒƒã‚°ã®å ´åˆã€credentials.json ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥èª­ã¿è¾¼ã‚€ã‹ã€ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        try:
            with open('credentials.json', 'r') as f:
                credentials = json.load(f)
            print("credentials.json ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        except Exception as e: 
            print(f"ã‚¨ãƒ©ãƒ¼: credentials.json ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚{e}")
            return
    else:
        try:
            credentials = json.loads(credentials_json_str)
            print("GCP_SERVICE_ACCOUNT_KEY ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        except json.JSONDecodeError as e:
            print(f"ã‚¨ãƒ©ãƒ¼: GCP_SERVICE_ACCOUNT_KEY ç’°å¢ƒå¤‰æ•°ã®JSONå½¢å¼ãŒä¸æ­£ã§ã™ã€‚{e}")
            return
    
    if not credentials:
        print("èªè¨¼æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    try:
        gc = gspread.service_account_from_dict(credentials)
        print("Google Sheets API èªè¨¼ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        
        sh = gc.open_by_key(spreadsheet_id)
        print(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ '{spreadsheet_id}' ã‚’é–‹ãã¾ã—ãŸã€‚")
        
        worksheet = sh.worksheet(worksheet_name)
        print(f"ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ '{worksheet_name}' ã‚’é¸æŠã—ã¾ã—ãŸã€‚")
        
        existing_data = worksheet.get_all_values()
        
        existing_urls = set()
        if len(existing_data) > 1:
            for row in existing_data[1:]:
                if len(row) > 1: 
                    existing_urls.add(row[1]) 
        
        print(f"æ—¢å­˜ã®ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ'{worksheet_name}'ã«ã¯ {len(existing_urls)} ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªURLãŒã‚ã‚Šã¾ã™ã€‚")

        data_to_append = []
        new_articles_count = 0

        # å–å¾—ã—ãŸè¨˜äº‹ã‚’æŠ•ç¨¿æ—¥ã§ã‚½ãƒ¼ãƒˆã™ã‚‹ï¼ˆæœ€æ–°ã®ã‚‚ã®ãŒä¸Šã«æ¥ã‚‹ã‚ˆã†ã«é™é †ï¼‰
        # 'æŠ•ç¨¿æ—¥'ã¯ 'YYYY/MM/DD HH:MM' å½¢å¼ãªã®ã§ã€datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã—ã¦ã‚½ãƒ¼ãƒˆ
        try:
            sorted_articles = sorted(
                articles, 
                key=lambda x: datetime.strptime(x.get('æŠ•ç¨¿æ—¥', '1900/01/01 00:00'), "%Y/%#m/%#d %H:%M"), 
                reverse=True
            )
        except Exception as e:
            print(f"è­¦å‘Š: æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼ã®ãŸã‚ã€ã‚½ãƒ¼ãƒˆãŒæ­£ã—ãè¡Œã‚ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™: {e}")
            sorted_articles = articles # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚½ãƒ¼ãƒˆã—ãªã„

        for article in sorted_articles:
            if article.get('URL') and article.get('URL') not in existing_urls:
                data_to_append.append([
                    article.get('ã‚¿ã‚¤ãƒˆãƒ«', ''),
                    article.get('URL', ''),
                    article.get('æŠ•ç¨¿æ—¥', ''),
                    article.get('å¼•ç”¨å…ƒ', '')
                ])
                new_articles_count += 1
                existing_urls.add(article.get('URL'))
            
        if data_to_append:
            worksheet.append_rows(data_to_append, value_input_option='USER_ENTERED')
            print(f"âœ… {new_articles_count}ä»¶ã®æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ'{worksheet_name}'ã«è¿½è¨˜ã—ã¾ã—ãŸã€‚")
        else:
            print(f"âš ï¸ ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ'{worksheet_name}'ã«æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
    except gspread.exceptions.SpreadsheetNotFound:
        print(f"âš ï¸ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ '{spreadsheet_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚IDã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        print("ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒä»˜ä¸ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except gspread.exceptions.APIError as e:
        print(f"âŒ Google Sheets API ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print(f"APIã‚¨ãƒ©ãƒ¼è©³ç´°: {e.response.text if hasattr(e, 'response') else 'è©³ç´°ä¸æ˜'}")
        print("ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æ¨©é™ã€ã¾ãŸã¯ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®å…±æœ‰è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        print(f"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


if __name__ == "__main__":
    print("ğŸš€ ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚’é–‹å§‹ã—ã¾ã™...")
    
    # --- Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã¨æ›¸ãè¾¼ã¿ ---
    print("\n--- Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ ---")
    google_news_articles = get_google_news_with_selenium(KEYWORD) 
    if google_news_articles:
        print(f"âœ¨ Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰ {len(google_news_articles)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        write_to_spreadsheet(google_news_articles, SPREADSHEET_ID, "Google") # "Google"ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã‚€
    else:
        print("ğŸ¤” Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    # --- Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã¨æ›¸ãè¾¼ã¿ ---
    print("\n--- Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ ---")
    yahoo_news_articles = get_yahoo_news_with_requests(KEYWORD)
    if yahoo_news_articles:
        print(f"âœ¨ Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰ {len(yahoo_news_articles)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        write_to_spreadsheet(yahoo_news_articles, SPREADSHEET_ID, "Yahoo") # "Yahoo"ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã‚€
    else:
        print("ğŸ¤” Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    print("\nâœ… å…¨ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒ»æ›¸ãè¾¼ã¿å®Œäº†")
