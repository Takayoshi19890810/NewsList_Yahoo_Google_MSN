import os
import json
import time
from datetime import datetime, timedelta
import re 

# --- Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ (Selenium) é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

# --- Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ (Requests + BeautifulSoup) é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
import requests
from bs4 import BeautifulSoup

# --- gspreadé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
import gspread

# å…±é€šè¨­å®š
KEYWORD = "æ—¥ç”£"
SPREADSHEET_ID = "1RglATeTbLU1SqlfXnNToJqhXLdNoHCdePldioKDQgU8" 


# --- Googleãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–¢æ•° (Selenium) ---
def get_google_news_with_selenium(keyword: str) -> list[dict]:
    """
    Seleniumã‚’ä½¿ç”¨ã—ã¦Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—ã—ã¾ã™ã€‚
    """
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080") 

    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        print("ChromeDriver launched.")
    except Exception as e:
        print(f"Error: Failed to launch ChromeDriver. {e}")
        return []

    url = f"https://news.google.com/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja"
    print(f"Accessing URL (Google): {url}")
    
    articles_data = []
    try:
        driver.get(url)
        time.sleep(5)

        for _ in range(3): # Scroll down multiple times to load dynamic content
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

        articles = driver.find_elements(By.TAG_NAME, "article")
        print(f"Number of article elements detected (Google): {len(articles)}")

        for i, article_elem in enumerate(articles):
            try:
                # Extract title, URL, published date, and source from each element
                title_tag = article_elem.find_element(By.CSS_SELECTOR, "a.JtKRv")
                date_tag = article_elem.find_element(By.CSS_SELECTOR, "time.hvbAAd")
                source_tag = article_elem.find_element(By.CSS_SELECTOR, "div.vr1PYe")

                title = title_tag.text.strip()
                date_utc_str = date_tag.get_attribute("datetime")
                url = title_tag.get_attribute("href")
                source = source_tag.text.strip() if source_tag else "N/A"

                if title and date_utc_str and url:
                    utc_dt = datetime.strptime(date_utc_str, "%Y-%m-%dT%H:%M:%SZ")
                    jst_dt = utc_dt + timedelta(hours=9) # Convert to JST
                    
                    formatted_date = f"{jst_dt.year}/{jst_dt.month}/{jst_dt.day} {jst_dt.hour:02}:{jst_dt.minute:02}"

                    full_url = "https://news.google.com" + url[1:] if url.startswith("./articles/") else url

                    articles_data.append({
                        'ã‚¿ã‚¤ãƒˆãƒ«': title,
                        'URL': full_url,
                        'æŠ•ç¨¿æ—¥': formatted_date,
                        'å¼•ç”¨å…ƒ': source
                    })
            except Exception as e:
                # print(f"Error parsing Google article element {i}: {e}") 
                continue
        return articles_data
    except Exception as e:
        print(f"Unexpected error occurred while retrieving Google News: {e}")
        return []
    finally:
        if driver:
            driver.quit()


# --- Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–¢æ•° (Requests + BeautifulSoup) ---
def get_yahoo_news_with_requests(keyword: str) -> list[dict]:
    """
    Requestsã¨BeautifulSoupã‚’ä½¿ç”¨ã—ã¦Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—ã—ã¾ã™ã€‚
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
    }
    
    url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    print(f"Accessing URL (Yahoo!): {url}")
    
    articles_data = []
    try:
        res = requests.get(url, headers=headers, timeout=10) 
        res.raise_for_status() 
        soup = BeautifulSoup(res.text, "html.parser")

        # Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ä¸»è¦ãªè¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã®ã‚»ãƒ¬ã‚¯ã‚¿å¼·åŒ–
        # ãƒ­ã‚°ã‹ã‚‰åˆ¤æ–­ã™ã‚‹ã«ã€ã„ãã¤ã‹ã®è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ã¯æ¤œå‡ºã•ã‚Œã¦ã„ã‚‹ãŒã€ãã®ä¸­ã®ã‚¿ã‚¤ãƒˆãƒ«/URLãŒå–ã‚Œã¦ã„ãªã„æ¨¡æ§˜
        article_blocks = soup.find_all("li", attrs={"data-tn-screen": "results-item"})
        if not article_blocks:
            article_blocks = soup.find_all("li", class_="newsFeed_item")
        if not article_blocks:
            # æ–°ã—ã„Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒŠã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹ã‚¯ãƒ©ã‚¹ã‚’è©¦ã™
            article_blocks = soup.find_all("div", class_=re.compile(r"sc-\w{6}-\d+\s+"))
        
        # ã•ã‚‰ã«æ±ç”¨çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ã‚‚è©¦ã™
        if not article_blocks:
            article_blocks = soup.find_all("div", class_="news-card") # Bing Newsã§ä½¿ã†ã‹ã‚‚ã—ã‚Œãªã„ãŒYahooã‚‚å…±é€šåŒ–ã—ã¦ã„ã‚‹å¯èƒ½æ€§
        if not article_blocks:
            article_blocks = soup.find_all("li", class_="articleListItem") # åˆ¥ã®ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆã‚¢ã‚¤ãƒ†ãƒ 


        if not article_blocks:
            print("âš ï¸ Yahoo! News: No article blocks found. Please check selectors.")
            return []

        print(f"Number of article elements detected (Yahoo!): {len(article_blocks)}")

        for i, article in enumerate(article_blocks):
            title = ""
            link = ""
            date_str = ""
            formatted_date = ""
            source_text = "N/A"

            try:
                # --- ã‚¿ã‚¤ãƒˆãƒ«ã¨URLã®æŠ½å‡ºå¼·åŒ– ---
                # æœ€ã‚‚ä¿¡é ¼æ€§ã®é«˜ã„ã‚»ãƒ¬ã‚¯ã‚¿ã‹ã‚‰é †ã«è©¦ã™
                # Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯aã‚¿ã‚°ç›´ä¸‹ã«ã‚¿ã‚¤ãƒˆãƒ«ãŒæ¥ã‚‹ã“ã¨ãŒå¤šã„
                title_link_tag = article.find("a", attrs={"data-cl-tab": "titleLink"}) # ç‰¹å®šã®å±æ€§ã‚’æŒã¤ãƒªãƒ³ã‚¯
                if not title_link_tag:
                    title_link_tag = article.find("a", class_="newsFeed_item_link") # ä¸€èˆ¬çš„ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ•ã‚£ãƒ¼ãƒ‰ã®ãƒªãƒ³ã‚¯ã‚¯ãƒ©ã‚¹
                if not title_link_tag:
                    title_link_tag = article.find("a", class_="newsArticle_link") # åˆ¥ã®ãƒªãƒ³ã‚¯ã‚¯ãƒ©ã‚¹
                if not title_link_tag:
                    title_link_tag = article.find("a", class_="sc-evrRku bVwJtJ") # ã‚¹ã‚¯ã‚·ãƒ§ãªã©ã‹ã‚‰è¦‹ã‚‰ã‚Œã‚‹ã‚¯ãƒ©ã‚¹
                if not title_link_tag:
                    title_link_tag = article.find("a", class_="newslink") # å¤ã„Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¯ãƒ©ã‚¹
                if not title_link_tag:
                    title_link_tag = article.find("h2").find("a", href=True) if article.find("h2") else None # h2ã‚¿ã‚°å†…ã®ãƒªãƒ³ã‚¯
                if not title_link_tag:
                    title_link_tag = article.find("h3").find("a", href=True) if article.find("h3") else None # h3ã‚¿ã‚°å†…ã®ãƒªãƒ³ã‚¯
                if not title_link_tag:
                    # è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯å†…ã®æœ€åˆã®æœ‰åŠ¹ãªãƒªãƒ³ã‚¯ã‚’è©¦ã™
                    first_link = article.find("a", href=True)
                    if first_link and "news.yahoo.co.jp" in first_link.get("href", ""):
                        title_link_tag = first_link

                if title_link_tag:
                    title = title_link_tag.text.strip()
                    link = title_link_tag.get("href", "")
                    
                    # åºƒå‘Šã‚„PRè¨˜äº‹ã‚’é™¤å¤–ã™ã‚‹ãŸã‚ã®è¿½åŠ ãƒã‚§ãƒƒã‚¯
                    if "pr-label" in article.get("class", []) or "sponsored" in link.lower() or "advertisement" in link.lower() or "pr_link" in link.lower():
                        print(f"  Article {i}: Skipping as it appears to be an ad/PR article.")
                        continue

                # --- æŠ•ç¨¿æ—¥ã®æŠ½å‡ºå¼·åŒ– ---
                time_tag = article.find("time")
                if time_tag:
                    date_str = time_tag.text.strip()
                    # (æ›œæ—¥) ã‚’å‰Šé™¤
                    date_str_clean = re.sub(r'\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)', '', date_str).strip() 
                    
                    try:
                        # "YYYY/MM/DD HH:MM" å½¢å¼
                        dt_obj = datetime.strptime(date_str_clean, "%Y/%m/%d %H:%M")
                        formatted_date = f"{dt_obj.year}/{dt_obj.month}/{dt_obj.day} {dt_obj.hour:02}:{dt_obj.minute:02}"
                    except ValueError:
                        try:
                            # "MM/DD HH:MM" å½¢å¼ (ä»Šå¹´ã®æ—¥ä»˜ã¨ä»®å®š)
                            current_year = datetime.now().year
                            dt_obj = datetime.strptime(f"{current_year}/{date_str_clean}", "%Y/%m/%d %H:%M")
                            formatted_date = f"{dt_obj.year}/{dt_obj.month}/{dt_obj.day} {dt_obj.hour:02}:{dt_obj.minute:02}"
                        except ValueError:
                            formatted_date = date_str_clean # å¤‰æ›ã§ããªã„å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨

                # --- å¼•ç”¨å…ƒã®æŠ½å‡ºå¼·åŒ– ---
                source_text = "N/A"
                source_tag_data_by_text = article.find("div", attrs={"data-by-text": True})
                if source_tag_data_by_text:
                    source_text = source_tag_data_by_text["data-by-text"].strip()
                else: 
                    # ã„ãã¤ã‹ã®ä¸€èˆ¬çš„ãªã‚½ãƒ¼ã‚¹ã‚¿ã‚°ã®ã‚¯ãƒ©ã‚¹ã‚’è©¦ã™
                    source_span = article.find("span", class_=re.compile(r"sc-\w{6}-\d+")) # å‹•çš„ã«å¤‰ã‚ã‚‹ã‚¯ãƒ©ã‚¹
                    if not source_span:
                        source_span = article.find("span", class_="newsFeed_item_media") # åˆ¥ã®ä¸€èˆ¬çš„ãªã‚½ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹
                    if not source_span:
                        source_span = article.find("span", class_="articleListItem_media") # åˆ¥ã®ãƒªã‚¹ãƒˆã‚¢ã‚¤ãƒ†ãƒ ã‚½ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹
                    if source_span:
                        source_text = source_span.text.strip()
                    else:
                        # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦ã€å°ã•ã„ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã‹ã‚‰æ¢ã™ï¼ˆãŸã ã—èª¤æ¤œçŸ¥ã®å¯èƒ½æ€§ã‚ã‚Šï¼‰
                        small_text_elements = article.find_all(lambda tag: tag.name in ['div', 'span', 'p'] and 'text-xs' in tag.get('class', []) or 'text-sm' in tag.get('class', []) or 'media' in tag.get('class', []))
                        for elem in small_text_elements:
                            text = elem.text.strip()
                            if 'è¨˜äº‹' not in text and 'PR' not in text and 'æä¾›' not in text and len(text) > 2 and len(text) < 50 and not re.match(r'\d{1,2}/\d{1,2}\(\w\)\s\d{1,2}:\d{2}', text) and not re.match(r'\d+æ™‚é–“å‰', text) and not re.match(r'\d+åˆ†å‰', text): # æ—¥ä»˜ã‚„æ™‚é–“ã§ãªã„ã“ã¨ã‚’ç¢ºèª
                                source_text = text
                                break


                print(f"  Article {i}: Title='{title}', URL='{link}', Date(raw)='{date_str}', Date(fmt)='{formatted_date}', Source='{source_text}'")

                if not title or not link:
                    print(f"  Article {i}: Skipping as title or URL is empty.")
                    continue


                articles_data.append({
                    'ã‚¿ã‚¤ãƒˆãƒ«': title,
                    'URL': link,
                    'æŠ•ç¨¿æ—¥': formatted_date,
                    'å¼•ç”¨å…ƒ': source_text
                })
                time.sleep(0.3) 
            except Exception as e:
                print(f"âŒ Error parsing Yahoo! article element {i}: {e}") 
                continue
        return articles_data
    except requests.exceptions.RequestException as e:
        print(f"Error during request to Yahoo! News: {e}")
        return []
    except Exception as e:
        print(f"Unexpected error occurred while retrieving Yahoo! News: {e}")
        return []


# --- MSNãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–¢æ•° (Selenium + BeautifulSoup) ---
def get_msn_news(keyword: str) -> list[dict]:
    """
    Seleniumã¨BeautifulSoupã‚’ä½¿ç”¨ã—ã¦MSNãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—ã—ã¾ã™ã€‚
    """
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080") 

    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        print("MSN ChromeDriver launched.")
    except Exception as e:
        print(f"Error: Failed to launch MSN ChromeDriver. {e}")
        return []

    now_jst = datetime.utcnow() + timedelta(hours=9)

    search_url = f'https://www.bing.com/news/search?q={keyword}&qft=sortbydate%3d"1"&form=YFNR'
    print(f"Accessing URL (MSN): {search_url}")
    
    articles_data = []
    try:
        driver.get(search_url)
        time.sleep(5)

        for _ in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
        
        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")

        news_cards = soup.select('div.news-card')
        print(f"Number of article elements detected (MSN): {len(news_cards)}")

        for i, card in enumerate(news_cards):
            title = card.get("data-title", "").strip()
            url = card.get("data-url", "").strip()
            source = card.get("data-author", "").strip() 

            pub_time_obj = None
            pub_label = ""

            pub_tag = card.find("span", attrs={"aria-label": True})
            if pub_tag and pub_tag.has_attr("aria-label"):
                pub_label = pub_tag["aria-label"].strip()

            if "åˆ†å‰" in pub_label:
                minutes_match = re.search(r"(\d+)", pub_label)
                if minutes_match:
                    minutes = int(minutes_match.group(1))
                    pub_time_obj = now_jst - timedelta(minutes=minutes)
            elif "æ™‚é–“å‰" in pub_label:
                hours_match = re.search(r"(\d+)", pub_label)
                if hours_match:
                    hours = int(hours_match.group(1))
                    pub_time_obj = now_jst - timedelta(hours=hours)
            elif "æ—¥å‰" in pub_label:
                days_match = re.search(r"(\d+)", pub_label)
                if days_match:
                    days = int(days_match.group(1))
                    pub_time_obj = now_jst - timedelta(days=days)
            else:
                try:
                    if re.match(r'\d+æœˆ\d+æ—¥', pub_label):
                        current_year = now_jst.year
                        date_str_with_year = f"{current_year}å¹´{pub_label}"
                        pub_time_obj = datetime.strptime(date_str_with_year, "%Yå¹´%mæœˆ%dæ—¥")
                    elif re.match(r'\d{4}/\d{1,2}/\d{1,2}', pub_label):
                        pub_time_obj = datetime.strptime(pub_label, "%Y/%m/%d")
                    elif re.match(r'\d{1,2}:\d{2}', pub_label): 
                        time_part = datetime.strptime(pub_label, "%H:%M").time()
                        pub_time_obj = datetime.combine(now_jst.date(), time_part)
                    else:
                        pub_time_obj = None
                except ValueError:
                    pub_time_obj = None

            formatted_date_str = pub_time_obj.strftime("%Y/%#m/%#d %H:%M") if pub_time_obj else pub_label

            print(f"  Article {i}: Title='{title}', URL='{url}', Date(raw)='{pub_label}', Date(fmt)='{formatted_date_str}', Source='{source}'")

            if title and url:
                articles_data.append({
                    'ã‚¿ã‚¤ãƒˆãƒ«': title,
                    'URL': url,
                    'æŠ•ç¨¿æ—¥': formatted_date_str, 
                    'å¼•ç”¨å…ƒ': source 
                })
            else:
                print(f"  Article {i}: Skipping as title or URL is empty.")

        return articles_data
    except Exception as e:
        print(f"MSNãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []
    finally:
        if driver:
            driver.quit()


# --- ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿é–¢æ•° ---
def write_to_spreadsheet(articles: list[dict], spreadsheet_id: str, worksheet_name: str):
    """
    å–å¾—ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿ã¾ã™ã€‚
    æ—¢å­˜ã®URLã‚’ãƒã‚§ãƒƒã‚¯ã—ã€æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’è¿½è¨˜ã—ã¾ã™ã€‚
    è¨˜äº‹ã¯æŠ•ç¨¿æ—¥ã®æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆã•ã‚Œã¦ã‹ã‚‰è¿½è¨˜ã•ã‚Œã¾ã™ã€‚

    Args:
        articles (list[dict]): æ›¸ãè¾¼ã‚€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ãƒªã‚¹ãƒˆã€‚
        spreadsheet_id (str): æ›¸ãè¾¼ã¿å…ˆã®Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã€‚
        worksheet_name (str): æ›¸ãè¾¼ã¿å…ˆã®ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆåã€‚
    """
    credentials_json_str = os.environ.get('GCP_SERVICE_ACCOUNT_KEY')
    credentials = None

    if credentials_json_str:
        try:
            credentials = json.loads(credentials_json_str)
            print("Loaded credentials from GCP_SERVICE_ACCOUNT_KEY environment variable.")
        except json.JSONDecodeError as e:
            print(f"Error: Invalid JSON format in GCP_SERVICE_ACCOUNT_KEY environment variable: {e}")
            return
    else:
        try:
            with open('credentials.json', 'r') as f:
                credentials = json.load(f)
            print("Loaded credentials from credentials.json file (for local execution).")
        except FileNotFoundError:
            print("âŒ Error: GCP_SERVICE_ACCOUNT_KEY environment variable is not set, and credentials.json was not found.")
            print("Please ensure you are running this in GitHub Actions, or place credentials.json for local execution.")
            return
        except json.JSONDecodeError as e:
            print(f"âŒ Error: Invalid format in credentials.json file: {e}")
            return
    
    if not credentials:
        print("Failed to obtain credentials. Skipping spreadsheet write operation.")
        return

    try:
        gc = gspread.service_account_from_dict(credentials)
        print("Google Sheets API authentication successful.")
        
        sh = gc.open_by_key(spreadsheet_id)
        print(f"Opened spreadsheet '{spreadsheet_id}'.")
        
        # ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        try:
            worksheet = sh.worksheet(worksheet_name)
            print(f"ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ '{worksheet_name}' ã‚’é¸æŠã—ã¾ã—ãŸã€‚")
        except gspread.exceptions.WorksheetNotFound:
            print(f"ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ '{worksheet_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’è€ƒæ…®ã—ã¦åˆæœŸè¡Œæ•°ã‚’è¨­å®š
            worksheet = sh.add_worksheet(title=worksheet_name, rows="1", cols="4") 
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ›¸ãè¾¼ã‚€
            worksheet.append_row(['ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'æŠ•ç¨¿æ—¥', 'å¼•ç”¨å…ƒ'])
            print(f"ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ '{worksheet_name}' ã‚’ä½œæˆã—ã€ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›¸ãè¾¼ã¿ã¾ã—ãŸã€‚")

        existing_data = worksheet.get_all_values()
        
        existing_urls = set()
        if len(existing_data) > 1: # Skip header row
            for row in existing_data[1:]:
                if len(row) > 1: # Ensure row has at least 2 columns (for URL)
                    existing_urls.add(row[1]) 
        
        print(f"Existing worksheet '{worksheet_name}' has {len(existing_urls)} unique URLs.")

        data_to_append = []
        new_articles_count = 0

        try:
            # 'æŠ•ç¨¿æ—¥'ãŒ 'YYYY/MM/DD HH:MM' å½¢å¼ã§ã‚ã‚‹ã“ã¨ã‚’å‰æ
            sorted_articles = sorted(
                articles, 
                key=lambda x: datetime.strptime(x.get('æŠ•ç¨¿æ—¥', '1900/01/01 00:00'), "%Y/%m/%d %H:%M"), 
                reverse=True # Newest articles first
            )
        except Exception as e:
            print(f"Warning: Date format error, sorting may not be correct: {e}")
            sorted_articles = articles # Use unsorted articles if error occurs

        for article in sorted_articles:
            if article.get('URL') and article.get('URL') not in existing_urls:
                data_to_append.append([
                    article.get('ã‚¿ã‚¤ãƒˆãƒ«', ''),
                    article.get('URL', ''),
                    article.get('æŠ•ç¨¿æ—¥', ''),
                    article.get('å¼•ç”¨å…ƒ', '')
                ])
                new_articles_count += 1
                existing_urls.add(article.get('URL')) 
            
        if data_to_append:
            worksheet.append_rows(data_to_append, value_input_option='USER_ENTERED')
            print(f"âœ… Appended {new_articles_count} new news data entries to worksheet '{worksheet_name}'.")
        else:
            print(f"âš ï¸ No new news data found for worksheet '{worksheet_name}'.")
            
    except gspread.exceptions.SpreadsheetNotFound:
        print(f"âš ï¸ Spreadsheet '{spreadsheet_id}' not found. Please check the ID.")
        print("Ensure the service account has access permission to the spreadsheet.")
    except gspread.exceptions.APIError as e:
        print(f"âŒ Google Sheets API ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print(f"API Error Details: {e.response.text if hasattr(e, 'response') else 'è©³ç´°ä¸æ˜'}")
        print("ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æ¨©é™ã€ã¾ãŸã¯ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®å…±æœ‰è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        print(f"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


if __name__ == "__main__":
    print("ğŸš€ ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚’é–‹å§‹ã—ã¾ã™...")
    
    # --- Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã¨æ›¸ãè¾¼ã¿ ---
    print("\n--- Google News ---")
    google_news_articles = get_google_news_with_selenium(KEYWORD) 
    if google_news_articles:
        print(f"âœ¨ Retrieved {len(google_news_articles)} news articles from Google News.")
        write_to_spreadsheet(google_news_articles, SPREADSHEET_ID, "Google") 
    else:
        print("ğŸ¤” Failed to retrieve Google News.")
    
    # --- Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã¨æ›¸ãè¾¼ã¿ ---
    print("\n--- Yahoo! News ---")
    yahoo_news_articles = get_yahoo_news_with_requests(KEYWORD)
    if yahoo_news_articles:
        print(f"âœ¨ Retrieved {len(yahoo_news_articles)} news articles from Yahoo! News.")
        write_to_spreadsheet(yahoo_news_articles, SPREADSHEET_ID, "Yahoo") 
    else:
        print("ğŸ¤” Failed to retrieve Yahoo! News.")

    # --- MSNãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã¨æ›¸ãè¾¼ã¿ ---
    print("\n--- MSN News ---")
    msn_news_articles = get_msn_news(KEYWORD)
    if msn_news_articles:
        print(f"âœ¨ Retrieved {len(msn_news_articles)} news articles from MSN News.")
        write_to_spreadsheet(msn_news_articles, SPREADSHEET_ID, "MSN") # "MSN"ã¨ã„ã†æ–°ã—ã„ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿
    else:
        print("ğŸ¤” Failed to retrieve MSN News.")
    
    print("\nâœ… All news retrieval and writing completed.")
